# Qwen 3.5 The GREATEST Opensource AI Model That Beats Opus 4.5 and Gemini 3? (Fully Tested)

- **頻道**：In The World of AI
- **平台**：youtube
- **時長**：14 分鐘
- **來源**：https://www.youtube.com/watch?v=TgZVAYXteIs
- **轉錄方式**：subtitle

## 轉錄內容

Looks like the Alibaba team is back with their new flagship open-source giant, and it's their first openweight model in the flagship lineup. This is where I would like to introduce the Quen 3.5 series from Alibaba, a massive 397 billion parameter model with 17 billion active parameters. It's natively multimodal and it's built as an all-purpose system. It's powered by a new architecture by combining hybrid linear attention with sparse mixture of experts and scaled with large reinforcement learning environments. And it's also designed for realworld agents. It also is reportedly 19 times faster than the Quen 3 Max which supports 201 languages and dialects. In terms of performance, it is quite strong. It records an 87.8 8 on the MMLU Pro and 87.5 on the video MME. It also beats Claude Opus 4.5 on browser comp as well as Gemini 3 Pro in several multimodal tasks. In terms of aa coding, it keeps its pace with Opus as well as surpassing Gemini 3 Pro on the sway bench verified test, though it trails on the terminal bench to Gemini. Overall, this model is shaping up to be one of the strongest openweight multimodal models yet. There's a lot of stuff in the AI space that I don't really put on to the YouTube channel, and you can actually access it through my free newsletter with the link in the description below where you can subscribe completely for free. This model is very strong as an open-source release cuz it is under the Apache 2.0, which is absolutely excellent for multimodal agent capabilities being open source. It also has great speed and efficiency, a 1 million context as well as a competitive model front that will be able to be used on different sorts of use cases. It's probably currently the best open- source multimodal model and it is something that can be used for vision plus reasoning as well as tool use quite well, but still has a lot of weaknesses in complex spatial tasks. code generation is decent, but it's not consistent. And the real world stability compared to top closed rivals is not there yet with the Quinn model series. I'm not saying it is horrible, but it is still one of the best open-source models yet, and it truly depends on the use case. You have a lot of ways to access this model since this is under the Apache 2.0 license. You can access the free weights of all of the different sizes. You can access it through their chatbot. You can get the API through their cloud service. And another way is using it through Kilo Code, which provides a free API up to $25 worth of free credits, as well as Open Router. Before we get started, allow me to introduce today's video sponsor, Mammoth AI. If you ever wished you could easily access every leading AI model in a single place, GPT, Gemini, Deepseek, Grock, whatever you name, Mammoth does exactly that. plus offers a powerful API that you can plug into tools like Pine, VS Code, NE, and many others. Right now in AI, we're seeing massive upgrades with Google's Gemini models known for huge context and multimodal reasoning. We also have Enthropic with its pod models, which are tuned for longer tasks like coding, spreadsheets, and research. And we also have emerging contenders like Deepseek or Kimi all launching new models within the next few months. All of these models are accessible and easily ready to use within Mammoth where you get access to many of these models in a single spot without switching APIs without any sort of hassle. It comes with a flexible pricing from just $10 a month. You can use it for deep research, code generation, multimodal workflow, document analysis, and custom AI projects. Use the link in the description below to get started. And thanks to Mammoth for supporting this channel. Just take a look at the Quinn 3.5 in action. This is where the demo is showcasing how the model was able to use React to build a live 3D mapbox. And this is a map that showcases Beijing as well as Shanghai. and it creates the interactive UI components which you can see here with different elements. You have real layouts and productionready front-end logic all generated autonomously. Overall, this is just showcasing how the model does a pretty good job with the real aentic multimodal coding aspect of this model, not just static code. Something to also highlight is that they also released the Quen 3 Next Coder Q8, which is a lean fast coding focus version of the same family. And in this case, it did a pretty great job with only eight blend parameters in terms of generating this Super Mario platformer game, which is pretty nice. You can see that it is functional. Added multiple components and did a pretty great job in terms of locally developing this. Now, apparently the coin model was able to generate this car racing game, and this probably looks really, really good. I'm not too sure if it was a oneshot generation. This is from the coin team. But still, regardless, it is great to see that a AI model was able to output the quality. I can't believe that this model was able to generate this. And I'm going to try my best to replicate this later on in the video, which you will see me trying to generate a car game to see if it was able to uphold the same sort of quality that you see within this demo. In Kilo Code, I had requested it to create a Mac OS browser operating system. And this is usually a test that I send into any model to test and evaluate how proficient it is obviously in front-end code but in dynamically using multiple agents to code out the back end. And it did a really great job in 20 cents which is quite cheap. It was able to generate this Mac OS operating system which is great but the only function of this is the appearance cuz I don't think a lot of the things actually work as great which is the only downside. But in terms of SVG icons, it did do a decent job. Now, the bottom toolbar looks pretty good. It is not going to allow me to open up anything, which is the only downside, but aside from that, it did do a decent job, I guess, in oneshotting this front end for this Mac OS operating system. I'm going to try my best to go back into Kilo to fix out all of these missing components that we're trying to look for. So I reprompted within Kilo and it was able to do a better job with the second generation and now we have functional components which is great. Each element has a highlight and the small things that mimic a Mac OS have been added with the second generation. But the only downside is is that none of these apps seem to be working aside from the finder cuz the Safari does work but everything else doesn't seem to be coded out yet. So this is the only downside when you use the minimized context. If you use the 1 million context with this model, it does a great job with coding. But when you're using the smaller tiered version, it's not going to get you the best output always. But still, it did a great job with this generation. Now, in my SVG prompts, I had first started off with the butterfly test, and this is where it did do a simple job in generating this butterfly. It even animated it, which is pretty nice to see. So, overall, it did pass the test, but nothing too extraordinary. This SVG generation of a butterfly where I told it to make it photo realistic is a lot better. And you can see that it did do a really great job with the photo realalistic butterfly. It even animated it which is great but it's not perfect which is the only downside cuz you can see that the right wing is overlapping the left one. But in terms of generating the photorealistic butterfly, it did do a great job and better than most open- source models. out with this prompt where I had requested it to create a premium front-end landing page. It did a faster job in generating this output within Arena versus the Opus 4.6 thinking obviously due to it being a reasoning model. Not saying that this model is not, but still it did a fast job in generating all the components that I'm looking for, the typography, the animations, and the dynamic elements of this landing page. It's not perfect by any means, but still it did a decent job with this front end, and I can say that this is a great alternative if you're looking to get fast and decent code outputed. Now, I tried the prompt again within the Quen chatbot, and this is the output of the SAS landing page I got, which is pretty great in my opinion. It was able to even create the image itself, which is nice, and the components of this does look great. Now, in my opinion, what it does look like is that these guys trained off of the Gemini output, which is something that a lot of these Chinese companies have been doing. I'm not shit-talking or anything, but I'm just stating the facts cuz I've seen this with all these different labs. I have personally talked to the people and they have stated clearly to me that they train their output off of these different proprietary models. I'm not saying it's wrong or anything. I'm just saying that this is what these companies are doing, which is kind of unfortunate cuz it'd be great to see the output that they're able to spin up with their own research and their own development. Next up is where we're going to be testing out the model's multimodal capabilities with a simple image prompt, a test to analyze how many cars there are, toy cars within this image. There are 1 2 3 4 5 6 7 1 2 3 4. So that's 28 cars in total right here within this image. We're trying to see if it's able to count all of these different cars. I have the prompt and image ready. Going to send it in with the thinking mode enabled. And you can see that it is going to reason which you can actually preview within the chatbot and it should output 28 which is perfect. So it did do a great job with this analysis. So remember we saw how it had generated that card game. Well, I tried it out and the first prompt that I had sent in was pretty bad in this case. I just told it create a car game and it outputed this. I'm not saying it's horrible, but it's not the same sort of quality, which is where then I went back into Kilo Code to ask for a better generation. And this is where I asked for a more detailed output. And this is what I had gotten. This is the Turbo Racer game that it had generated. Now, just take a look at the quality of output. Now I will say this is a decent generation but it does not mimic what we had saw from the Quen demos which might have been obviously prompted and edited multiple times to get the output. In this case we got this super basic generation. And maybe my prompting was wrong, but this is the output I had gotten. Definitely not the same quality, but still it is a great functional game that I had outputed. My next prompt, I had requested it to create a 3D room designer tool. And this is where you have the ability to add in different furnitureures to visualize how they actually look. You can change the wall colors as well, which you can see the changing in the background, which is really nice to see. Now, if I am to add in multiple things, it's not going to be able to allow me to move it, which is the only downside. But the spatial recognition of this design is perfect. And it does allow me to add in all these different components directly to our uh 3D room designer, which is nice to see. The bookshelf doesn't look as great, but everything else does look pretty great in terms of mimicking actual furniture. Now, it did do a great job in also showcasing how the light mode works, which is great, but it doesn't showcase how the elements are affected inside the room, which is the only issue. But the night mode, when you are to minimize the lighting, it does reflect how the lamp actually reflects warm light, which is great to see. Uh, other than that, the code architecture looks pretty decent. Visual 3D as well as code logic was outputed and it did do a decent job with the interactivity here. In my next prompt, I had requested it to create a video. So, just take a look at this output. >> What do you want to say? >> You first. >> Now, that's pretty decent in my opinion. If you like this video and would love to support the channel, you can consider donating to my channel through the super thanks option below. Or you can consider joining our private Discord where you can access multiple subscriptions to different AI tools for free on a monthly basis, plus daily AI news and exclusive content, plus a lot more here. In a single shot, it was able to create a farming simulation game that kind of mimics Stardew Valley, and it did do a decent job with this generation. And I got to say the output over here is pretty qualitative where you have multiple functions like harvesting, planting, you have interactions with different entities like the animals and overall it did output the production ready demo logic with movements. You have the inventory, crop timers and a lot of the other components. So this is a pretty decent game that it had outputed. Overall, this is a great remarkable multimodal model and it's something that you can potentially use for realworld agents. I definitely love the pricing for this model based off of the quality that you get as well as the efficiency faster than most models which is nice to see and providing decent uh output. Now it still obviously has its weaknesses and complex spatial task like I showcased as well as stability compared to the top rivals but still a great decent model that I would just use as a backup I would say. Nothing too great about it, but as a local model, it would be a great alternative to like many of the other open source models we see. But let me know what you guys think. I want to definitely hear your thoughts in the comment section below. Make sure you take a look at all the links that I put in today's video in the description. Make sure you join the newsletter as well as our second channel. Join the Patreon, the Discord, make sure you join the Twitter, and lastly, make sure you guys subscribe, turn on the notification bell, like this video, and please take a look at our previous videos so that you can stay up to date with the latest AI news. But with that thought, guys, have an amazing day. Spread positivity. Pray that we see Gemini 3.5 soon as well as Sonic 5. And have an amazing day. He saw us.
